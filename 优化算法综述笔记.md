### 导言：

梯度下降的简要解释：

- 目标函数：$J(\theta)\qquad \theta\in\mathbb{R}^d$ 
- 优化方式：向梯度$\nabla_\theta J(\theta)$的反方向更新参数
- 学习率：$\eta$ 决定了我们的步长
- 换句话说，我们沿着目标函数创建的曲面斜率的方向向下走，直到我们到达一个山谷

### 梯度下降起源（小论文报告）

1. 总结。设M(x)表示某一实验关于x的期望值。M(x)被假定为x的单调函数，但实验者不知道，希望找到方程$M(x) = \alpha$的解$x = \theta$，其中$\alpha$是给定常数。我们给出了一种在第$x_1,x_2,\cdots,x_n$级进行连续实验的方法，使$x_n$在概率上趋向于$\theta$。

### 梯度下降变体

有三种梯度下降算法，不同点是我们在一次迭代中选择多少数据进行梯度的计算

#### Batch gradient descent

$$
\theta = \theta-\eta\,\cdot\,\nabla_\theta J(\theta)
$$



- 选择全部样本进行更新
- 能保证收敛于局部最小值（凸函数可全局）

#### Stochastic gradient descent

$$
\theta=\theta-\eta\cdot\nabla_\theta J(\theta;x^{(i)};y^{(i)})
$$

- 对每一个样本都执行一次更新
- $x^{(i)}$：样本    $y^{(i)}$：标签
- 更新时会波动，但更新更快，也能进入新的局部最小值
- `loss:0.051, train_acc:0.988, test_acc:0.884 200.examples/sec on cuda:0`

#### Mini-batch gradient descent

综合前两种的优点，对每n个训练示例进行一次更新，n即为batch size
$$
\theta=\theta-\eta\cdot\nabla_\theta J(\theta;x^{(i:i+n)};y^{(i:i+n)})
$$

- （相较于SGD）减少了参数更新的方差，收敛更稳定
- 提供了高度的并行性，能够使用GPU以及当前深度学习库的高度优化矩阵计算
- `loss:0.051, train_acc:0.988, test_acc:0.884
  12924.0 examples/sec on cuda:0
  116.06s total`

### 梯度下降带来的挑战

- **学习率的选择**：太小会收敛缓慢，太大会导致损失函数波动甚至不能收敛
- 学习率时间规则在调整学习率上做了一些贡献，它尝试在训练期间通过退火来调整学习率，调整规则预先定义；能在一定程度上缓解问题，但由于整个过程仍然是提前定义的，因此不能完全适应所有应用，还是比较麻烦。
- **参数频率问题**：SGD当中，学习率用于所有的参数更新，如果我们的数据是稀疏的，特征频率差别大，一些特征经常被值为0的数据抹掉，导致更新次数很少，我们希望对这类特征执行更大的更新。
- **鞍点**：即一个维度向上倾斜，另一个维度向下倾斜的点。这些鞍点通常被具有相同误差的平台所包围，这使得SGD难以逃脱，因为在所有维度上梯度都接近于零。

### 梯度下降的优化

#### SGD with Momentum

- 在局部最优值附近SGD容易发生振荡，而只在底部朝着局部最优方向犹豫不决地前进。

- **momentum**用来加速梯度方向上的参数更新，能够一定程度上抑制震荡。

- 实现方式：添加参数$\gamma$ ，每次更新时都拿上一步更新的向量的$\gamma$倍添加到本次更新的向量上，$\gamma$一般取0.9
  $$
  \begin{array}{l}v_t=\gamma v_{t-1}+\eta\nabla_\theta J(\theta)\\ \theta=\theta-v_t\end{array}
  $$

- 动量项对于梯度指向相同方向的维度增加，对于梯度改变方向的维度减小，因此能够更快的收敛、减少振荡。

- `loss:0.009, train_acc:0.997, test_acc:0.900
  12524.9 examples/sec on cuda:0
  119.76s total`

#### Nesterov accelerated gradient

严格来讲，动量方法中的式子即
$$
\theta=\theta-\gamma v_{t-1}-\eta \nabla_\theta J(\theta)
$$

- 实现方式：通过减去动量项给$\theta$了一个下一时刻的近似位置（还没有减梯度），更准确的更新是，在近似位置上计算梯度，而不是在原来的位置，即

$$
\begin{array}{l}v_t=\gamma v_{t-1}+\eta\nabla_\theta J(\theta-\gamma v_{t-1})\\ \theta=\theta-v_t\end{array}
$$

- 现在，我们能够根据误差函数的梯度调整更新，从而加快SGD的速度.
- `loss:0.006, train_acc:0.998, test_acc:0.898
  12468.4 examples/sec on cuda:0
  120.30s total`

#### Adagrad

Adagrad能够根据参数动态调整学习率，对低频出现的参数执行较大更新，对高频出现的参数执行较小的更新

- 先前我们进行一次更新时，对每一个参数$\theta_i$都使用相同的学习率$\eta$，而Adagrad时间步不同、参数不同则学习率也不同，因此我们定义$g_{t,i}$为目标函数关于时间步$t$和参数$\theta_i$的梯度，即
  $$
  g_{t,i}=\nabla_{\theta_t}J(\theta_{t,i})
  $$

- 更新方式为
  $$
  \theta_{t+1,i}=\theta_{t,i}-\dfrac{\eta}{\sqrt{G_{t,ii}+\epsilon}}\cdot g_{t,i}
  $$
  其中$G_t\in\mathbb{R}^{d\times d}$ 是一个对角阵，对角线上第i个值为第i个参数从开始到t个时间步的梯度的平方和，即
  $$
  G_{t,ii}=\Sigma_{j=1}^tg_{j,i}^2\quad\qquad i=1,2,\cdots,d
  $$
  $\epsilon$是一个极小量，避免分母为0

- 将更新方式写为向量形式，即
  $$
  \begin{align}
  \theta_{t+1}=\theta_t-\dfrac{\eta}{\sqrt{G_t+\epsilon}}\odot g_t 
  \end{align}
  $$
  
- Adagrad最大的优点是他不需要再手动的调整学习率，大多数实现都使用默认值0.01

- 缺点是G的累积都是正值，其和在训练过程中不断增长，最终学习率会变得无穷小，导致算法不能进行更新，Adadelta即要解决这一问题。

- `loss:0.004, train_acc:1.000, test_acc:0.894
  12391.6 examples/sec on cuda:0
  121.05s total`

#### Adadelta

Adadelta会控制积累过去梯度的大小，不会无限制的积累，从而避免学习率无穷小的问题。**窗口大小由t变为一个固定的w，即每次只看（累加）前面w步的梯度，而不是所有梯度都看；而储存w个梯度是低效的，实现变成了计算衰减平均。**确切的说，梯度和被递归定义为过去所有平方梯度的衰减平均值，t时刻的平均值仅依赖于$\gamma$倍的之前的平均值和当前梯度
$$
E[g^2]_t=\gamma E[g^2]_{t-1}+(1-\gamma)g_t^2
$$
只需要把Adagrad向量更新公式中的$G_t$换为$E[g^2]_{t-1}$，就得到了Adadelta的更新公式，即
$$
\Delta\theta_t=-\dfrac{\eta}{\sqrt{E[g^2]_t+\epsilon}}g_t
$$
其中分母是梯度的均方根（RMS）误差准则，我们将其简写为
$$
\Delta\theta_t=-\dfrac{\eta}{RMS[g]_t}g_t
$$

- 作者指出，此更新中的单位(以及SGD、Momentum或Adagrad)不匹配，此前的更新均是参数加梯度，这在具有单位的场景中并不合适；即更新应该具有与参数相同而不是梯度的假设单位。为了实现它，首先要定义关于参数的衰减平均与均方根：
  $$
  \begin{align}
  E[\Delta\theta^2]_t&=\gamma E[\Delta\theta^2]_{t-1}+(1-\gamma)\Delta\theta^2_t\\
  RMS[\Delta\theta]_t&=\sqrt{E[\Delta\theta^2]_t+\epsilon}
  \end{align}
  $$

- 随后将更新的二阶方法做一个变换得到新的梯度更新方式
  $$
  \begin{align}
  \Delta \theta\propto H^{-1}g\propto\frac{\frac{\partial J}{\partial \theta}}{\frac{\partial^2J}{\partial \theta^2}}\propto\text{units of }\theta\\
  \Delta \theta=\frac{\frac{\partial J}{\partial \theta}}{\frac{\partial^2J}{\partial \theta^2}}\Rightarrow\frac{1}{\frac{\partial^2J}{\partial \theta^2}}=\frac{\Delta \theta}{\frac{\partial J}{\partial \theta}}
  \end{align}
  $$

- 下面近似梯度与参数，即用均方根近似，由于当前时刻$\Delta\theta_t$未知，我们用t-1时刻的参数均方根来近似，最终
  $$
  \begin{aligned}&\Delta\theta_t=-\frac{RMS[\Delta\theta]_{t-1}}{RMS[g]_t}g_t\\ &\theta_{t+1}=\theta_t+\Delta\theta_t\end{aligned}
  $$
  即为更新方式，我们甚至不需要设置学习率。
  
- `loss:0.012, train_acc:0.997, test_acc:0.900
  11373.6 examples/sec on cuda:0
  131.88s total`

#### RMSprop

即为一部分的Adadelta，Hinton没有将其发表
$$
\begin{aligned}E[g^2]_t&=0.9E[g^2]_{t-1}+0.1g^2_t\\ \theta_{t+1}&=\theta_t-\frac{\eta}{\sqrt{E[g^2]_t+\epsilon}}g_t\end{aligned}
$$

`loss:0.035, train_acc:0.988, test_acc:0.874
12215.8 examples/sec on cuda:0
122.79s total`

学习率0.1

#### Adam

自适应矩估计（Adaptive Moment Estimation），目前最主流的优化方法，比Adadelta多了一项梯度的指数衰减平均（Adadelta是梯度平方的指数衰减平均）：
$$
\begin{array}{c}m_t=\beta_1m_{t-1}+(1-\beta_1)g_t\\ v_t=\beta_2v_{t-1}+(1-\beta_2)g_t^2\end{array}
$$
$m_t$和$v_t$分别是$g_t$的第一矩（均值）估计和第二矩（非中心方差）估计；作者注意到了一个问题并给出了解决方案：当两向量初始化为0时，他们在初始或者$\beta_1\text{、}\beta_2$接近于1时偏向于0；他们通过计算经过偏差校正的第一和第二矩估计来抵消偏差：
$$
\begin{aligned}\hat{m}_t&=\frac{m_t}{1-\beta_1^t}\\ \hat{v}_t&=\frac{v_t}{1-\beta_2^t}\end{aligned}
$$
Adam的更新规则如下：
$$
\theta_{t+1}=\theta_t-\dfrac{\eta}{\sqrt{\hat{v}_t}+\epsilon}\hat{m}_t
$$
The authors propose default values of 0.9 for $\beta_1$, 0.999 for $β_2$, and 10−8 for $\epsilon$.

结果表明，Adam算法在实践中表现良好，优于其他自适应学习算法。

`loss:0.035, train_acc:0.987, test_acc:0.901
11679.9 examples/sec on cuda:0
128.43s total`

#### AdaMax

在Adam的更新规则中，$v_t$的更新规则为
$$
v_t=\beta_2v_{t-1}+(1-\beta_2)|g_t|^2
$$
AdaMax将这个更新扩展到了p范数，同时将$\beta_2\text{变成了}\beta_2^p$：
$$
v_t=\beta_2^p v_{t-1}+(1-\beta_2^p)|g_t|^p
$$
p范数通常p的值越大，在数值上会变得越不稳定，这就是为什么“1”和“2”范数在实践中最常见。但是，$∞$范数通常也表现出稳定的行为。为此，作者提出了AdaMax，并证明了$v_t$在$∞$时收敛到以下更稳定的值。为了避免与Adam混淆，我们用$u_t$表示无限范数约束的$v_t$:
$$
\begin{aligned}u_t=\lim_{p\to\infty}(v_t)^{1/p}&=\lim_{p\to\infty}\left((1-\beta_2^p)\sum_{i=1}^t\beta_2^{p(t-i)}\cdot|g_i|^p\right)^{1/p}\\ &=\lim_{p\to\infty}(1-\beta_2^{p})^{1/p}\left(\sum_{i=1}^t\beta_2^{p(t-i)}\cdot|g_i|^p\right)^{1/p}\\
&=\lim_{p\to\infty}\left(\sum_{i=1}^t\left(\beta_2^{(t-i)}\cdot|g_i|\right)^p\right)^{1/p}\\ 
&=\max\left(\beta_2^{t-1}|g_1|,\beta_2^{t-2}|g_2|,\ldots,\beta_2|g_{t-1}|,|g_t|\right)\end{aligned}
$$
简写为
$$
u_t=\max(\beta_2\cdot u_{t-1},|g_t|)
$$
更新方式变为了
$$
\theta_{t+1}=\theta_t-\dfrac{\eta}{u_t}\hat{m}_t
$$
$u_t$不需要偏差校正，因为他依赖于max运算，不会很容易偏向0. 
Good default values are again η = 0.002, β1 = 0.9, and β2 = 0.999.

`loss:0.021, train_acc:0.993, test_acc:0.900
11803.6 examples/sec on cuda:0
127.08s total`

#### Nadam

Adam可以被视为是RMSprop与momentum的组合，即梯度平方衰减平均+梯度衰减平军；之前提到Nesterov加速梯度性能上是优于momentum的；基于此提出了Nadam，融合Adam与NAG。

注意到NAG需要修改梯度的计算，因此我们需要调整Adam的动量项$m_t$，注意到之前SGD with momentum的NAG优化中，更新规则为：
$$
\begin{aligned}g_t&=\nabla_{\theta_t}J(\theta_t-\gamma m_{t-1})\\ m_t&=\gamma m_{t-1}+\eta g_t\\ \theta_{t+1}&=\theta_t-m_t\end{aligned}
$$
该更新规则中，动量项m被应用了两次，一次计算梯度，一次更新参数；Nadam将其简化为一次，使用当前时间步的m直接更新参数，具体见下述更新规则：
$$
\begin{aligned}g_t&=\nabla_{\theta_t}J(\theta_t)\\ m_t&=\gamma m_{t-1}+\eta g_t\\ \theta_{t+1}&=\theta_t-(\gamma m_t+\eta g_t)\end{aligned}
$$
对比SGD with momentum的更新：$\theta_{t+1}=\theta_t-(\gamma m_{t-1}+\eta g_t)$，可以发现仅仅是$m_t \text{与}m_{t-1}$的差别
我们首先给出Adam的更新规则（已将动量项更新代入）
$$
\begin{align}
\theta_{t+1}&=\theta_t-\dfrac{\eta}{\sqrt{\hat{v}_t}+\epsilon}(\dfrac{\beta_1m_{t-1}}{1-\beta_1^t}+\dfrac{(1-\beta_1)g_t}{1-\beta_1^t})\\
&=\theta_{t}-\frac{\eta}{\sqrt{\hat{v}_{t}}+\epsilon}(\beta_{1}\hat{m}_{t-1}+\frac{(1-\beta_{1})g_{t}}{1-\beta_{1}^{t}})
\end{align}
$$
该方程与动量SGD的更新公式十分相似，我们现在做一样的事情（即把$m_{t-1}\text{代换为}m_t$，即得出Nadam的更新规则：
$$
\theta_{t+1}=\theta_t-\dfrac{\eta}{\sqrt{\hat{v}_t}+\epsilon}(\beta_1\hat{m}_t+\dfrac{(1-\beta_1)g_t}{1-\beta_1^t})
$$
`loss:0.036, train_acc:0.986, test_acc:0.891
11310.4 examples/sec on cuda:0
132.62s total`

`epoch 1: 7.76s
epoch 2: 4.46s
epoch 3: 4.47s
epoch 4: 4.50s
epoch 5: 4.51s
loss:0.317, train_acc:0.896, test_acc:0.858
11670.2 examples/sec on cuda:0
25.71s total
epoch 1: 4.72s
epoch 2: 4.60s
epoch 3: 4.61s
epoch 4: 4.59s
epoch 5: 4.61s
loss:0.184, train_acc:0.935, test_acc:0.874
12973.0 examples/sec on cuda:0
23.12s total
epoch 1: 4.75s
epoch 2: 4.66s
epoch 3: 4.65s
epoch 4: 4.65s
epoch 5: 4.67s
loss:0.179, train_acc:0.936, test_acc:0.866
12829.8 examples/sec on cuda:0
23.38s total
epoch 1: 4.79s
epoch 2: 4.68s
epoch 3: 4.74s
epoch 4: 4.73s
epoch 5: 4.70s`





`loss:0.174, train_acc:0.939, test_acc:0.891
12690.9 examples/sec on cuda:0
23.64s total
epoch 1: 4.99s
epoch 2: 4.99s
epoch 3: 4.78s
epoch 4: 4.82s
epoch 5: 4.86s
loss:0.204, train_acc:0.925, test_acc:0.877
12275.3 examples/sec on cuda:0
24.44s total
epoch 1: 5.16s
epoch 2: 5.06s
epoch 3: 5.09s
epoch 4: 5.09s
epoch 5: 5.05s
loss:0.194, train_acc:0.932, test_acc:0.854
11790.4 examples/sec on cuda:0
25.44s total
epoch 1: 5.12s
epoch 2: 5.00s
epoch 3: 5.05s
epoch 4: 5.18s
epoch 5: 5.03s
loss:0.214, train_acc:0.921, test_acc:0.879
11817.2 examples/sec on cuda:0
25.39s total
epoch 1: 5.14s
epoch 2: 5.05s
epoch 3: 5.04s
epoch 4: 4.99s
epoch 5: 4.95s
loss:0.206, train_acc:0.924, test_acc:0.890
11917.8 examples/sec on cuda:0
25.17s total
epoch 1: 5.30s
epoch 2: 5.21s
epoch 3: 5.22s
epoch 4: 5.21s
epoch 5: 5.23s
loss:0.227, train_acc:0.916, test_acc:0.838
11465.5 examples/sec on cuda:0
26.17s total`
